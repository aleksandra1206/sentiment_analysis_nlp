{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksandra1206/sentiment_analysis_nlp/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89673f05",
      "metadata": {
        "id": "89673f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e448e565-e92b-473d-a3bb-9c6d5225040b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b3d7d7",
      "metadata": {
        "id": "78b3d7d7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Domaci2BD/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CACBnZQ4e80n",
      "metadata": {
        "id": "CACBnZQ4e80n"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('TwitterSentimentAnalysis').config(\"spark.executor.instances\", \"4\").config(\"spark.executor.cores\", \"8\").config(\"spark.executor.memory\", \"18g\").config(\"spark.driver.memory\", \"8g\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CLUkARsqMrfp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLUkARsqMrfp",
        "outputId": "49472779-2260-4ae4-bbe7-3e1f30e8ba4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id  |Title      |Sentiment|Sentence                                                                                                                                                                                                                                                                                             |\n",
            "+----+-----------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2401|Borderlands|Positive |im getting on borderlands and i will murder you all ,                                                                                                                                                                                                                                                |\n",
            "|2401|Borderlands|Positive |I am coming to the borders and I will kill you all,                                                                                                                                                                                                                                                  |\n",
            "|2401|Borderlands|Positive |im getting on borderlands and i will kill you all,                                                                                                                                                                                                                                                   |\n",
            "|2401|Borderlands|Positive |im coming on borderlands and i will murder you all,                                                                                                                                                                                                                                                  |\n",
            "|2401|Borderlands|Positive |im getting on borderlands 2 and i will murder you me all,                                                                                                                                                                                                                                            |\n",
            "|2401|Borderlands|Positive |im getting into borderlands and i can murder you all,                                                                                                                                                                                                                                                |\n",
            "|2402|Borderlands|Positive |So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg       |\n",
            "|2402|Borderlands|Positive |So I spent a couple of hours doing something for fun... If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters, I decided to make a wallpaper for my PC.. Here's the original picture compared to the creation I made:) Have fun! pic.twitter.com / mLsI5wf9Jg|\n",
            "|2402|Borderlands|Positive |So I spent a few hours doing something for fun... If you don't know I'm a HUGE @ Borderlands fan and Maya is one of my favorite characters.                                                                                                                                                          |\n",
            "|2402|Borderlands|Positive |So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg          |\n",
            "|2402|Borderlands|Positive |2010 So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg     |\n",
            "|2402|Borderlands|Positive |was                                                                                                                                                                                                                                                                                                  |\n",
            "|2403|Borderlands|Neutral  |Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it/RMTrgF                                                                                                                                                                                                         |\n",
            "|2403|Borderlands|Neutral  |Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it / RMTrgF                                                                                                                                                                                                       |\n",
            "|2403|Borderlands|Neutral  |Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dfr.it / RMTrgF                                                                                                                                                                                                        |\n",
            "|2403|Borderlands|Neutral  |Rock-Hard La Vita, RARE BUT POWERFUL, HANDSOME JACKPOT, Borderlands 1 (Xbox) dlvr.it/RMTrgF                                                                                                                                                                                                          |\n",
            "|2403|Borderlands|Neutral  |Live Rock - Hard music La la Varlope, RARE & the POWERFUL, Live HANDSOME i JACKPOT, Borderlands 3 ( Sega Xbox ) dlvr. From it / e RMTrgF                                                                                                                                                             |\n",
            "|2403|Borderlands|Neutral  |I-Hard like me, RARE LONDON DE, HANDSOME 2011, Borderlands 3 (Xbox) dlvr.it/RMTrgF                                                                                                                                                                                                                   |\n",
            "|2404|Borderlands|Positive |that was the first borderlands session in a long time where i actually had a really satisfying combat experience. i got some really good kills                                                                                                                                                       |\n",
            "|2404|Borderlands|Positive |this was the first Borderlands session in a long time where i actually had a really satisfying fighting experience. i got some really good kills                                                                                                                                                     |\n",
            "+----+-----------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"Title\", StringType(), True),\n",
        "    StructField(\"Sentiment\", StringType(), True),\n",
        "    StructField(\"Sentence\", StringType(), True)\n",
        "])\n",
        "\n",
        "data = spark.read.csv(data_path+\"twitter_training.csv\", schema=schema, header=False)\n",
        "\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yh2Bhhh6Dk-e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh2Bhhh6Dk-e",
        "outputId": "65ab6f48-b09e-4c15-e24a-351bbc8f4ca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broj redova gde je Sentence null:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "686"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"Broj redova gde je Sentence null:\")\n",
        "data.filter(data[\"Sentence\"].isNull()).count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# izbacivanje redova gde je Sentence null\n",
        "print(\"Broj redova gde je Sentence null nakon filtriranja:\")\n",
        "data = data.na.drop(subset=[\"Sentence\"])\n",
        "data.filter(data[\"Sentence\"].isNull()).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0OCjmnYP23C",
        "outputId": "08a1ce31-716c-4bad-b793-447011cb3ad2"
      },
      "id": "Z0OCjmnYP23C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broj redova gde je Sentence null nakon filtriranja:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fo-G1Uc6b9yx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo-G1Uc6b9yx",
        "outputId": "1013d5bd-a429-4a27-9e5c-7549ed360c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtrirani podaci bez null vrednosti, broj redova:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73996"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "print(\"Filtrirani podaci bez null vrednosti, broj redova:\")\n",
        "data.count()\n",
        "# dataset ima 73996 redova i moze usporiti performanse modela, zbog cega ce se koristiti stratifikovano uzorkovanje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bHdVthluxM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1bHdVthluxM",
        "outputId": "1200fc4a-7377-42d9-bc4e-6fa00894eca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Udeo i broj svakog sentimenta u celom dataset-u: \n",
            "+----------+-----+-------------------+\n",
            "| Sentiment|count|           frakcija|\n",
            "+----------+-----+-------------------+\n",
            "|Irrelevant|12875|   0.17399589166982|\n",
            "|  Positive|20655|0.27913671009243746|\n",
            "|   Neutral|18108|0.24471593059084276|\n",
            "|  Negative|22358| 0.3021514676468998|\n",
            "+----------+-----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "sentiment_counts = data.groupBy(\"Sentiment\").count().withColumn(\"frakcija\", col(\"count\") / data.count())\n",
        "print(\"Udeo i broj svakog sentimenta u celom dataset-u: \")\n",
        "sentiment_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1j3hdr2SlyqD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j3hdr2SlyqD",
        "outputId": "61fa6967-6ef4-49fb-cd4f-0f36c93ea650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smanjen skup:\n",
            "+----------+-----+-------------------+\n",
            "| Sentiment|count|           frakcija|\n",
            "+----------+-----+-------------------+\n",
            "|Irrelevant|   20|0.21505376344086022|\n",
            "|   Neutral|   23|0.24731182795698925|\n",
            "|  Positive|   27| 0.2903225806451613|\n",
            "|  Negative|   23|0.24731182795698925|\n",
            "+----------+-----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SMANJIVANJE DATASET-A\n",
        "rowNum = 100\n",
        "fractions = {row['Sentiment']: rowNum / data.count() for row in sentiment_counts.collect()}\n",
        "\n",
        "\n",
        "# Stratifikovano uzorkovanje koristeći frakcije\n",
        "sampled = data.stat.sampleBy(\"Sentiment\", fractions, seed=40)\n",
        "\n",
        "print(\"Smanjen skup:\")\n",
        "new_sentiment = sampled.groupBy(\"Sentiment\").count().withColumn(\"frakcija\", col(\"count\") / sampled.count())\n",
        "new_sentiment.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "li-CdmJXCvyE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li-CdmJXCvyE",
        "outputId": "9862c25a-a8af-4f27-ab15-ec0a89b0e564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data = sampled\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, size\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, IndexToString,NGram\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cor7aLxeUSBn",
      "metadata": {
        "id": "Cor7aLxeUSBn"
      },
      "outputs": [],
      "source": [
        "# Parser za lematizaciju\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"]) # disable parser i ner zbog efikasnosti\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_and_lemmatize_partition(partition):\n",
        "    texts = []\n",
        "    sentiments = []\n",
        "\n",
        "    for row in partition:\n",
        "        tokens = nltk.word_tokenize(row['Sentence']) # od teksta u svakom redu izvuci tokene\n",
        "        texts.append(' '.join(tokens))\n",
        "        sentiments.append(row['Sentiment']) # dodaj sentiment za dati red tokena\n",
        "\n",
        "    # Procesuirati tekst pomocu spacy pipe\n",
        "    lemmatized_texts = []\n",
        "    for doc in spacy_nlp.pipe(texts, batch_size=50):\n",
        "        lemmatized_texts.append([token.lemma_ for token in doc]) # vrati lemu za svaki token\n",
        "\n",
        "    # Vrati rezultat\n",
        "    for sentiment, lemmatized in zip(sentiments, lemmatized_texts):\n",
        "        yield (sentiment, lemmatized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AI2tTnERMg9B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI2tTnERMg9B",
        "outputId": "3917ed83-5274-4f57-b2be-024809c73165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset nakon tokenizacije, lemitizacije i uklanjanja stop reci:\n",
            "+----------+--------------------+-----+--------------------+\n",
            "| Sentiment|          lemmatized|Index|            filtered|\n",
            "+----------+--------------------+-----+--------------------+\n",
            "|Irrelevant|             [S4, c]|  3.0|             [S4, c]|\n",
            "|  Positive|[I, ', m, actuall...|  0.0|[', m, actually, ...|\n",
            "|   Neutral|[nice, video, by,...|  2.0|[nice, video, @, ...|\n",
            "|   Neutral|[I, do, not, know...|  2.0|[know, ,, somehow...|\n",
            "|  Positive|[lol, I, be, look...|  0.0|[lol, look, somet...|\n",
            "|  Negative|            [wtf, ?]|  1.0|            [wtf, ?]|\n",
            "|  Negative|[pilot, see, when...|  1.0|[pilot, see, usel...|\n",
            "|  Positive|       [handy, work]|  0.0|       [handy, work]|\n",
            "|   Neutral|[<, unk, >, just,...|  2.0|[<, unk, >, get, ...|\n",
            "|  Negative|[@, Verizon, @, y...|  1.0|[@, Verizon, @, y...|\n",
            "|  Negative|[sweet, !, more, ...|  1.0|[sweet, !, recycl...|\n",
            "|Irrelevant|[FICK, you, @, sc...|  3.0|[FICK, @, scroffy...|\n",
            "|Irrelevant|[HELLS, yes, !, !...|  3.0|[HELLS, yes, !, !...|\n",
            "|  Negative|[play, league, of...|  1.0|[play, league, le...|\n",
            "|   Neutral|[Microsoft, on, W...|  2.0|[Microsoft, Wedne...|\n",
            "|   Neutral|[MELUSI, o, be, y...|  2.0|[MELUSI, o, shock...|\n",
            "|Irrelevant|[if, only, I, hav...|  3.0|         [online, .]|\n",
            "|   Neutral|[Baker, Avenue, A...|  2.0|[Baker, Avenue, A...|\n",
            "|Irrelevant|[fucking, my, buddy]|  3.0|    [fucking, buddy]|\n",
            "|   Neutral|[thank, Kimoon, f...|  2.0|[thank, Kimoon, a...|\n",
            "+----------+--------------------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data = data.repartition(5)\n",
        "data = data.rdd.mapPartitions(tokenize_and_lemmatize_partition).toDF([\"Sentiment\", \"lemmatized\"])\n",
        "\n",
        "# Napraviti indeks od kolone Sentiment\n",
        "stringIndexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"Index\")\n",
        "data = stringIndexer.fit(data).transform(data)\n",
        "\n",
        "# Ciscenje stop reci\n",
        "stopWordsRemover = StopWordsRemover(inputCol=\"lemmatized\", outputCol=\"filtered\")\n",
        "data = stopWordsRemover.transform(data)\n",
        "\n",
        "# izbaci sve redove gde nakon uklanjanja stop reci ne ostaje ni jedna rec u filter koloni\n",
        "data = data.filter(size(col(\"filtered\")) > 0)\n",
        "\n",
        "# Prikazi rezultat\n",
        "print(\"Dataset nakon tokenizacije, lemitizacije i uklanjanja stop reci:\")\n",
        "data.select(\"Sentiment\", \"lemmatized\", \"Index\", \"filtered\").show()\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QPzCx_rui8fP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPzCx_rui8fP",
        "outputId": "f8ff52fb-2caf-4a97-fa58-90742be9228d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trening podaci:\n",
            "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Sentiment |lemmatized                                                                                                                                                                                      |Index|filtered                                                                                                                                      |\n",
            "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Irrelevant|[HELLS, yes, !, !, !, so, PUMPED, to, FINALLY, see, some, VAMPIRE, BLOODLINES, 2, .]                                                                                                            |3.0  |[HELLS, yes, !, !, !, PUMPED, FINALLY, see, VAMPIRE, BLOODLINES, 2, .]                                                                        |\n",
            "|Irrelevant|[S4, c]                                                                                                                                                                                         |3.0  |[S4, c]                                                                                                                                       |\n",
            "|Irrelevant|[fucking, my, buddy]                                                                                                                                                                            |3.0  |[fucking, buddy]                                                                                                                              |\n",
            "|Irrelevant|[if, only, I, have, online, .]                                                                                                                                                                  |3.0  |[online, .]                                                                                                                                   |\n",
            "|Negative  |[play, league, of, legend, on, your, own, must, be, one, my, bad, idea]                                                                                                                         |1.0  |[play, league, legend, must, one, bad, idea]                                                                                                  |\n",
            "|Negative  |[sweet, !, more, recycled, content, !, nothing, excite, I, more, than, pay, $, 60, for, a, game, I, ', ve, already, play, !, bo1, remastered, will, be, awesome, !]                             |1.0  |[sweet, !, recycled, content, !, nothing, excite, pay, $, 60, game, ', ve, already, play, !, bo1, remastered, awesome, !]                     |\n",
            "|Negative  |[wtf, ?]                                                                                                                                                                                        |1.0  |[wtf, ?]                                                                                                                                      |\n",
            "|Neutral   |[<, unk, >, just, get, the, [, the, most, Horrific, Vision, of, Stormwind, ], achievement, !]                                                                                                   |2.0  |[<, unk, >, get, [, Horrific, Vision, Stormwind, ], achievement, !]                                                                           |\n",
            "|Neutral   |[Baker, Avenue, Asset, Management, LP, grow, in, Johnson, &, Johnson, $, JNJ, modernreaders.com, /, ?, p, =, 3866369]                                                                           |2.0  |[Baker, Avenue, Asset, Management, LP, grow, Johnson, &, Johnson, $, JNJ, modernreaders.com, /, ?, p, =, 3866369]                             |\n",
            "|Neutral   |[I, do, not, know, ,, somehow, it, remind, I, of, a, wifi, router, ..., I, can, not, wait, to, watch, gameplay, video, .]                                                                       |2.0  |[know, ,, somehow, remind, wifi, router, ..., wait, watch, gameplay, video, .]                                                                |\n",
            "|Neutral   |[MELUSI, o, be, you, shocked, !, @, Justhoneybadger, @, simmyster06, @, walkerpool, @, Mdassassin007, @, owahid65]                                                                              |2.0  |[MELUSI, o, shocked, !, @, Justhoneybadger, @, simmyster06, @, walkerpool, @, Mdassassin007, @, owahid65]                                     |\n",
            "|Neutral   |[nice, video, by, @, JeffGrubb, explain, the, Auto, -, HDR, function, of, the, .., have, a, look, at, the, video, below, :, ., youtube.com, /, watch, ?, v, =, z9nx15, ...]                     |2.0  |[nice, video, @, JeffGrubb, explain, Auto, -, HDR, function, .., look, video, :, ., youtube.com, /, watch, ?, v, =, z9nx15, ...]              |\n",
            "|Positive  |[I, ', m, actually, very, impatiently, wait, to, play]                                                                                                                                          |0.0  |[', m, actually, impatiently, wait, play]                                                                                                     |\n",
            "|Positive  |[handy, work]                                                                                                                                                                                   |0.0  |[handy, work]                                                                                                                                 |\n",
            "|Irrelevant|[a, ban, for, Battlefield, 4, player, Scoutcopter, have, occur, SEE, DETAILS, :, bf4db.com/player/ban/189, …]                                                                                   |3.0  |[ban, Battlefield, 4, player, Scoutcopter, occur, SEE, DETAILS, :, bf4db.com/player/ban/189, …]                                               |\n",
            "|Negative  |[Fortnite, still, have, a, place, in, all, my, heart, ,, but, damn, it, ', if, s, never, go, downhill]                                                                                          |1.0  |[Fortnite, still, place, heart, ,, damn, ', never, go, downhill]                                                                              |\n",
            "|Negative  |[fuck, you, @, apexlegend]                                                                                                                                                                      |1.0  |[fuck, @, apexlegend]                                                                                                                         |\n",
            "|Neutral   |[Nutz, Factory, open, soon, !, go, with, some, live, ., twitch.tv, /, deezopnutz, .]                                                                                                            |2.0  |[Nutz, Factory, open, soon, !, go, live, ., twitch.tv, /, deezopnutz, .]                                                                      |\n",
            "|Neutral   |[Red, Dead, Redemption, 2, !, now, live, on, twitch.tv/marcel_kiefer]                                                                                                                           |2.0  |[Red, Dead, Redemption, 2, !, live, twitch.tv/marcel_kiefer]                                                                                  |\n",
            "|Neutral   |[if, we, ever, get, Battlefield, 6, I, do, hope, it, be, as, simplistic, as, Battlefield, 4, be, ., instead, of, that, overly, annoying, gameplay, and, class, restriction, Battlefield, 5, get]|2.0  |[ever, get, Battlefield, 6, hope, simplistic, Battlefield, 4, ., instead, overly, annoying, gameplay, class, restriction, Battlefield, 5, get]|\n",
            "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test podaci:\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Sentiment |lemmatized                                                                                                                                                                                                                                                                                        |Index|filtered                                                                                                                                                                                                                |\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Irrelevant|[FICK, you, @, scroffy, _, _, U, FKN, jinxed, it]                                                                                                                                                                                                                                                 |3.0  |[FICK, @, scroffy, _, _, U, FKN, jinxed]                                                                                                                                                                                |\n",
            "|Negative  |[@, Verizon, @, yofGilroy, ,, as, soon, as, we, leave, town, ,, 5, g, speed, exceed, 50, Mbit, /, s, in, town, ?, abysmal, ..., Block, of, Camron, Blvd, ,, Gilroy, ,, CA, speedtest.net, /, my, -, result, /, I, /, 42, ...]                                                                     |1.0  |[@, Verizon, @, yofGilroy, ,, soon, leave, town, ,, 5, g, speed, exceed, 50, Mbit, /, town, ?, abysmal, ..., Block, Camron, Blvd, ,, Gilroy, ,, CA, speedtest.net, /, -, result, /, /, 42, ...]                         |\n",
            "|Negative  |[pilot, see, when, these, useless, @, valveso2, @, dota2updates, admin, do, something, to, the, smurf, .]                                                                                                                                                                                         |1.0  |[pilot, see, useless, @, valveso2, @, dota2updates, admin, something, smurf, .]                                                                                                                                         |\n",
            "|Neutral   |[Microsoft, on, Wednesday, report, rise, margin, in, the, past, quarter, amid, strong, demand, for, cloud, computing, service, and, pandemic, -, hit, business, as, consumer, and, big, gain, within, its, Xbox, gaming, operation, ., bit.ly/2D8bxdR]                                            |2.0  |[Microsoft, Wednesday, report, rise, margin, past, quarter, amid, strong, demand, cloud, computing, service, pandemic, -, hit, business, consumer, big, gain, within, Xbox, gaming, operation, ., bit.ly/2D8bxdR]       |\n",
            "|Positive  |[lol, I, be, look, for, something, to, listen, to, that, be, upbeat, for, 1998, and, the, first, recommendation, be, my, heart, will, go, on, ,, nice, job, google]                                                                                                                               |0.0  |[lol, look, something, listen, upbeat, 1998, first, recommendation, heart, go, ,, nice, job, google]                                                                                                                    |\n",
            "|Negative  |[@, CfDuty, either, need, to, turn, off, match, -, making, base, on, skill, in, warzone, or, allow, cross, -, play, to, be, turn, off, ., I, ', m, not, try, to, play, against, a, lobby, fill, with, hard, computer, player, in, every, game, ..., this, be, a, quick, way, to, destroy, warzone]|1.0  |[@, CfDuty, either, need, turn, match, -, making, base, skill, warzone, allow, cross, -, play, turn, ., ', m, try, play, lobby, fill, hard, computer, player, every, game, ..., quick, way, destroy, warzone]           |\n",
            "|Neutral   |[just, earn, my, creativity, in, the, classroom, on, the, Microsoft, Educator, Challenge, !, you, n, check, out, some, PD, course, :, education.microsoft.com/en-us/learne, …, via, @, MicrosoftEDU]                                                                                              |2.0  |[earn, creativity, classroom, Microsoft, Educator, Challenge, !, n, check, PD, course, :, education.microsoft.com/en-us/learne, …, via, @, MicrosoftEDU]                                                                |\n",
            "|Positive  |[..., this, official, Microsoft, Program, Trailer, Book, of, ., youtube, ., com, /, watch, ?, v, =, gkibn, …]                                                                                                                                                                                     |0.0  |[..., official, Microsoft, Program, Trailer, Book, ., youtube, ., com, /, watch, ?, v, =, gkibn, …]                                                                                                                     |\n",
            "|Positive  |[I, ', m, very, impatiently, try, to, play]                                                                                                                                                                                                                                                       |0.0  |[', m, impatiently, try, play]                                                                                                                                                                                          |\n",
            "|Positive  |[I, ', ve, be, think, about, GTA, IV, recently, and, what, a, bang, game, it, be, ...., -, with, one, of, the, good, theme, song, of, any, generation, .......]                                                                                                                                   |0.0  |[', ve, think, GTA, IV, recently, bang, game, ...., -, one, good, theme, song, generation, .......]                                                                                                                     |\n",
            "|Negative  |[my, dick, be, microsoft]                                                                                                                                                                                                                                                                         |1.0  |[dick, microsoft]                                                                                                                                                                                                       |\n",
            "|Irrelevant|[thc, 2008, -, Fortnite, Montage/, Highlights, -, the, Happy, Campers, GG, sigmamist.com/blog/thc-loox-, …, https, :, //t.co, /, fRlwY5dQNC, ]]                                                                                                                                                   |3.0  |[thc, 2008, -, Fortnite, Montage/, Highlights, -, Happy, Campers, GG, sigmamist.com/blog/thc-loox-, …, https, :, //t.co, /, fRlwY5dQNC, ]]                                                                              |\n",
            "|Positive  |[it, be, nice, to, go, back, and, use, this, ..., store.playstation.com, /]                                                                                                                                                                                                                       |0.0  |[nice, go, back, use, ..., store.playstation.com, /]                                                                                                                                                                    |\n",
            "|Irrelevant|[the, unstoppable, sweeping, Bayern, Munich, on, fifa]                                                                                                                                                                                                                                            |3.0  |[unstoppable, sweeping, Bayern, Munich, fifa]                                                                                                                                                                           |\n",
            "|Negative  |[hell, ,, even, Verizon, Wireless, leave, I, on, an, open, SMH, .]                                                                                                                                                                                                                                |1.0  |[hell, ,, even, Verizon, Wireless, leave, open, SMH, .]                                                                                                                                                                 |\n",
            "|Negative  |[if, GTA, 6, be, even, close, to, the, quality, and, scope, of, RDR2, ,, then, you, should, not, expect, it, for, a, while, yet, .]                                                                                                                                                               |1.0  |[GTA, 6, even, close, quality, scope, RDR2, ,, expect, yet, .]                                                                                                                                                          |\n",
            "|Negative  |[it, be, not, the, first, time, that, the, EU, Commission, have, take, such, a, step, .]                                                                                                                                                                                                          |1.0  |[first, time, EU, Commission, take, step, .]                                                                                                                                                                            |\n",
            "|Neutral   |[Amazon, UK, launch, the, Sherlock, Holmes, Advent, calendar, -, amzn.to, /, 3jh8yzn, and, amzn.to, /, 2z4ynlf, -, last, year, the, calendar, be, so, sell, out, that, it, be, worth, get, they, early, !, https, :, /, /, t.co, /, geynuqu9mv]                                                   |2.0  |[Amazon, UK, launch, Sherlock, Holmes, Advent, calendar, -, amzn.to, /, 3jh8yzn, amzn.to, /, 2z4ynlf, -, last, year, calendar, sell, worth, get, early, !, https, :, /, /, t.co, /, geynuqu9mv]                         |\n",
            "|Neutral   |[Geek, Fam, win, one, Esports, Dota, 2, SEA, League, :, Geek, Fam, finish, a, strong, low, bracket, run, with, a, 3, -, 1, victory, over, tournament, favorite, Fnatic, to, win, one, Esports, Dota, ..., esportspocket.com/dota2/geek-fam, …]                                                    |2.0  |[Geek, Fam, win, one, Esports, Dota, 2, SEA, League, :, Geek, Fam, finish, strong, low, bracket, run, 3, -, 1, victory, tournament, favorite, Fnatic, win, one, Esports, Dota, ..., esportspocket.com/dota2/geek-fam, …]|\n",
            "|Positive  |[dope, 5, come, on, @, fazesway, you, be, dope, wtf, ..]                                                                                                                                                                                                                                          |0.0  |[dope, 5, come, @, fazesway, dope, wtf, ..]                                                                                                                                                                             |\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=43)\n",
        "\n",
        "print(\"Trening podaci:\")\n",
        "train_data.show(truncate=False)\n",
        "\n",
        "print(\"Test podaci:\")\n",
        "test_data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rIBaF5s_hnBr",
      "metadata": {
        "id": "rIBaF5s_hnBr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, NGram, Word2Vec\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "def create_pipeline(text_method, model):\n",
        "    stages = []\n",
        "    if text_method == \"hashingTF idf\":\n",
        "        hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
        "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "        stages.extend([hashingTF, idf])\n",
        "    elif text_method == \"ngram hashingTF idf\":\n",
        "        ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
        "        hashingTF = HashingTF(inputCol=\"bigrams\", outputCol=\"rawFeaturesBigrams\")\n",
        "        idf = IDF(inputCol=\"rawFeaturesBigrams\", outputCol=\"features\")\n",
        "        stages.extend([ngram, hashingTF, idf])\n",
        "    elif text_method == \"word2vec\":\n",
        "        word2Vec = Word2Vec(inputCol=\"filtered\", outputCol=\"features\", vectorSize=3, minCount=0)\n",
        "        stages.append(word2Vec)\n",
        "\n",
        "    stages.append(model)\n",
        "\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XZGNHwNNhop6",
      "metadata": {
        "id": "XZGNHwNNhop6"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"Index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"Index\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"Index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "def train(text_method, models, train_data):\n",
        "    trained_models = []\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Training {model_name} with {text_method}...\")\n",
        "\n",
        "        # Kreiranje pipeline-a\n",
        "        pipeline = create_pipeline(text_method, model)\n",
        "\n",
        "        # Definisan parametarski grid za hiperparametre\n",
        "        paramGrid = ParamGridBuilder()\n",
        "        if model_name == \"LogisticRegression\":\n",
        "            paramGrid.addGrid(model.regParam, [0.1, 0.01])\n",
        "            paramGrid.addGrid(model.elasticNetParam, [0.0, 0.5])\n",
        "        elif model_name == \"DecisionTreeClassifier\":\n",
        "            paramGrid.addGrid(model.maxDepth, [5, 10])\n",
        "            paramGrid.addGrid(model.minInstancesPerNode, [1, 2])\n",
        "        elif model_name == \"RandomForestClassifier\":\n",
        "            paramGrid.addGrid(model.numTrees, [20, 50])\n",
        "\n",
        "        paramGrid = paramGrid.build()\n",
        "\n",
        "        # Definisan CrossValidator\n",
        "        crossval = CrossValidator(estimator=pipeline,\n",
        "                                  estimatorParamMaps=paramGrid,\n",
        "                                  evaluator=evaluator_accuracy,\n",
        "                                  numFolds=3)\n",
        "\n",
        "        # Obuka modela\n",
        "        cv_model = crossval.fit(train_data)\n",
        "        trained_models.append((model_name + \" with \" + text_method, cv_model.bestModel))\n",
        "\n",
        "        print(f\"Training completed for {model_name} with {text_method}\")\n",
        "\n",
        "    return trained_models\n",
        "\n",
        "def learn(trained_models, test_data):\n",
        "    print(\"Evaluating models...\")\n",
        "    results = {}\n",
        "    for model_name, bestModel in trained_models:\n",
        "        if model_name not in results:\n",
        "            results[model_name] = []\n",
        "        # Predikcija na test podacima\n",
        "        predictions = bestModel.transform(test_data)\n",
        "\n",
        "        # Evaluacija performansi modela\n",
        "        f1 = evaluator_f1.evaluate(predictions)\n",
        "        accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "        precision = evaluator_precision.evaluate(predictions)\n",
        "        recall = evaluator_recall.evaluate(predictions)\n",
        "        bestModelParams = {param.name: value for param, value in bestModel.stages[-1].extractParamMap().items()}\n",
        "        results[model_name].append({\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Parameters\": bestModelParams\n",
        "        })\n",
        "    return results\n",
        "# Lista metoda obrade teksta\n",
        "text_methods = [\"hashingTF idf\", \"ngram hashingTF idf\", \"word2vec\"]\n",
        "\n",
        "# Lista modela\n",
        "models = {\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Index\"),\n",
        "    \"LogisticRegression\": LogisticRegression(featuresCol=\"features\", labelCol=\"Index\"),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"Index\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wU5yFnfcrMPO",
      "metadata": {
        "id": "wU5yFnfcrMPO"
      },
      "outputs": [],
      "source": [
        "# Iteracija kroz metode obrade teksta\n",
        "def train_models(text_methods, models, train_data):\n",
        "  trained_models = []\n",
        "  for text_method in text_methods:\n",
        "      # Treniranje svih modela za dati tekst metod i cuvanje u dictionary\n",
        "      trained_models=trained_models + train(text_method, models, train_data)\n",
        "  return trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3o88RtCDgogc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o88RtCDgogc",
        "outputId": "85df655b-1b63-4dc4-f42b-d5bcb1be6c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DecisionTreeClassifier with hashingTF idf...\n",
            "Training completed for DecisionTreeClassifier with hashingTF idf\n",
            "Training LogisticRegression with hashingTF idf...\n",
            "Training completed for LogisticRegression with hashingTF idf\n",
            "Training RandomForestClassifier with hashingTF idf...\n",
            "Training completed for RandomForestClassifier with hashingTF idf\n",
            "Training DecisionTreeClassifier with ngram hashingTF idf...\n",
            "Training completed for DecisionTreeClassifier with ngram hashingTF idf\n",
            "Training LogisticRegression with ngram hashingTF idf...\n",
            "Training completed for LogisticRegression with ngram hashingTF idf\n",
            "Training RandomForestClassifier with ngram hashingTF idf...\n",
            "Training completed for RandomForestClassifier with ngram hashingTF idf\n",
            "Training DecisionTreeClassifier with word2vec...\n",
            "Training completed for DecisionTreeClassifier with word2vec\n",
            "Training LogisticRegression with word2vec...\n",
            "Training completed for LogisticRegression with word2vec\n",
            "Training RandomForestClassifier with word2vec...\n",
            "Training completed for RandomForestClassifier with word2vec\n"
          ]
        }
      ],
      "source": [
        "trained_models = train_models(text_methods, models, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluacija svih treniranih modela\n",
        "result = learn(trained_models, test_data)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3C27z59tPea",
        "outputId": "31785d37-f53e-4107-87ef-e9077393ede2"
      },
      "id": "I3C27z59tPea",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating models...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DecisionTreeClassifier with hashingTF idf': [{'Accuracy': 0.2,\n",
              "   'Precision': 0.10428571428571429,\n",
              "   'Recall': 0.2,\n",
              "   'F1 Score': 0.13444444444444445,\n",
              "   'Parameters': {'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': 5165298090080506173}}],\n",
              " 'LogisticRegression with hashingTF idf': [{'Accuracy': 0.15,\n",
              "   'Precision': 0.08787878787878788,\n",
              "   'Recall': 0.15000000000000002,\n",
              "   'F1 Score': 0.11058823529411765,\n",
              "   'Parameters': {'aggregationDepth': 2,\n",
              "    'elasticNetParam': 0.5,\n",
              "    'family': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'fitIntercept': True,\n",
              "    'labelCol': 'Index',\n",
              "    'maxBlockSizeInMB': 0.0,\n",
              "    'maxIter': 100,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'regParam': 0.1,\n",
              "    'standardization': True,\n",
              "    'threshold': 0.5,\n",
              "    'tol': 1e-06}}],\n",
              " 'RandomForestClassifier with hashingTF idf': [{'Accuracy': 0.3,\n",
              "   'Precision': 0.09,\n",
              "   'Recall': 0.3,\n",
              "   'F1 Score': 0.13846153846153847,\n",
              "   'Parameters': {'bootstrap': True,\n",
              "    'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featureSubsetStrategy': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'numTrees': 20,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': -8071993258401203131,\n",
              "    'subsamplingRate': 1.0}}],\n",
              " 'DecisionTreeClassifier with ngram hashingTF idf': [{'Accuracy': 0.25,\n",
              "   'Precision': 0.19444444444444442,\n",
              "   'Recall': 0.25,\n",
              "   'F1 Score': 0.1477272727272727,\n",
              "   'Parameters': {'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': 5165298090080506173}}],\n",
              " 'LogisticRegression with ngram hashingTF idf': [{'Accuracy': 0.25,\n",
              "   'Precision': 0.07894736842105263,\n",
              "   'Recall': 0.25,\n",
              "   'F1 Score': 0.12,\n",
              "   'Parameters': {'aggregationDepth': 2,\n",
              "    'elasticNetParam': 0.0,\n",
              "    'family': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'fitIntercept': True,\n",
              "    'labelCol': 'Index',\n",
              "    'maxBlockSizeInMB': 0.0,\n",
              "    'maxIter': 100,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'regParam': 0.1,\n",
              "    'standardization': True,\n",
              "    'threshold': 0.5,\n",
              "    'tol': 1e-06}}],\n",
              " 'RandomForestClassifier with ngram hashingTF idf': [{'Accuracy': 0.2,\n",
              "   'Precision': 0.06666666666666667,\n",
              "   'Recall': 0.2,\n",
              "   'F1 Score': 0.1,\n",
              "   'Parameters': {'bootstrap': True,\n",
              "    'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featureSubsetStrategy': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'numTrees': 50,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': -8071993258401203131,\n",
              "    'subsamplingRate': 1.0}}],\n",
              " 'DecisionTreeClassifier with word2vec': [{'Accuracy': 0.25,\n",
              "   'Precision': 0.47115384615384615,\n",
              "   'Recall': 0.25,\n",
              "   'F1 Score': 0.21808823529411764,\n",
              "   'Parameters': {'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': 5165298090080506173}}],\n",
              " 'LogisticRegression with word2vec': [{'Accuracy': 0.35,\n",
              "   'Precision': 0.2175824175824176,\n",
              "   'Recall': 0.35,\n",
              "   'F1 Score': 0.25520361990950224,\n",
              "   'Parameters': {'aggregationDepth': 2,\n",
              "    'elasticNetParam': 0.5,\n",
              "    'family': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'fitIntercept': True,\n",
              "    'labelCol': 'Index',\n",
              "    'maxBlockSizeInMB': 0.0,\n",
              "    'maxIter': 100,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'regParam': 0.1,\n",
              "    'standardization': True,\n",
              "    'threshold': 0.5,\n",
              "    'tol': 1e-06}}],\n",
              " 'RandomForestClassifier with word2vec': [{'Accuracy': 0.3,\n",
              "   'Precision': 0.3595238095238095,\n",
              "   'Recall': 0.30000000000000004,\n",
              "   'F1 Score': 0.27,\n",
              "   'Parameters': {'bootstrap': True,\n",
              "    'cacheNodeIds': False,\n",
              "    'checkpointInterval': 10,\n",
              "    'featureSubsetStrategy': 'auto',\n",
              "    'featuresCol': 'features',\n",
              "    'impurity': 'gini',\n",
              "    'labelCol': 'Index',\n",
              "    'leafCol': '',\n",
              "    'maxBins': 32,\n",
              "    'maxDepth': 5,\n",
              "    'maxMemoryInMB': 256,\n",
              "    'minInfoGain': 0.0,\n",
              "    'minInstancesPerNode': 1,\n",
              "    'minWeightFractionPerNode': 0.0,\n",
              "    'numTrees': 20,\n",
              "    'predictionCol': 'prediction',\n",
              "    'probabilityCol': 'probability',\n",
              "    'rawPredictionCol': 'rawPrediction',\n",
              "    'seed': -8071993258401203131,\n",
              "    'subsamplingRate': 1.0}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}